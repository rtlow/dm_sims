{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:/run_2cDM_L3N128_HY_power00_sigma1', 'D:/run_2cDM_L3N256_HY_power00_sigma1', 'D:/run_2cDM_L3N256_HY_power00_sigma1_dir_0', 'D:/run_2cDM_L3N256_HY_power00_sigma1_dir_1', 'D:/run_2cDM_L3N256_HY_power00_sigma1_dir_2', 'D:/run_2cDM_L3N256_HY_power00_sigma1_dir_3', 'D:/run_2cDM_L3N256_HY_power00_sigma1_dir_4', 'D:/run_2cDM_L3N256_HY_power00_sigma1_dir_5', 'D:/run_2cDM_L3N256_HY_power00_sigma1_dir_6', 'D:/run_2cDM_L3N256_HY_power00_sigma1_dir_7', 'D:/run_2cDM_L3N256_HY_power00_sigma1_dir_8', 'D:/run_2cDM_L3N256_HY_power00_sigma1_dir_9', 'D:/run_2cDM_L3N256_HY_powerm2m2_sigma1', 'D:/run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_0', 'D:/run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_1', 'D:/run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_2', 'D:/run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_3', 'D:/run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_4', 'D:/run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_5', 'D:/run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_6', 'D:/run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_7', 'D:/run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_8', 'D:/run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_9', 'D:/run_CDM_L3N128_HY', 'D:/run_CDM_L3N256_HY', 'D:/run_CDM_L3N256_HY_dir_0', 'D:/run_CDM_L3N256_HY_dir_1', 'D:/run_CDM_L3N256_HY_dir_2', 'D:/run_CDM_L3N256_HY_dir_3', 'D:/run_CDM_L3N256_HY_dir_4', 'D:/run_CDM_L3N256_HY_dir_5', 'D:/run_CDM_L3N256_HY_dir_6', 'D:/run_CDM_L3N256_HY_dir_7', 'D:/run_CDM_L3N256_HY_dir_8', 'D:/run_CDM_L3N256_HY_dir_9', 'D:/System Volume Information']\n"
     ]
    }
   ],
   "source": [
    "outdir = '../../data_prods/'\n",
    "#data_dir = '/home/ryan/Data'\n",
    "data_dir = 'D:/'\n",
    "subdir_list = [os.path.join(data_dir, o) for o in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir,o))]\n",
    "\n",
    "print(subdir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_crit = 2.7754*10**11 # solar mass / Mpc^3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parsePower(powerType):\n",
    "    stripped = powerType[5:]\n",
    "    negFlag = False\n",
    "    powers = [] \n",
    "    for char in stripped:\n",
    "        if char == 'm':\n",
    "            negFlag = True\n",
    "            continue\n",
    "        if negFlag:\n",
    "            powers.append(-int(char))\n",
    "            negFlag = False\n",
    "        else:\n",
    "            powers.append(int(char))\n",
    "\n",
    "    return powers\n",
    "\n",
    "def parseSigma(sigmaType):\n",
    "    return float(sigmaType[5:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_profiles(f, nbins=1000):\n",
    "\n",
    "    Subhalo=f.get('Subhalo')\n",
    "    SubhaloMass=np.array(Subhalo['SubhaloMass'])\n",
    "    SubhaloVMax=np.array(Subhalo['SubhaloVmax']) #km/s\n",
    "    MaxVel=np.absolute(SubhaloVMax)\n",
    "    SubhaloMass=10**10*SubhaloMass #converting to solar masses\n",
    "    SubhaloN = Subhalo['SubhaloLen'][()]\n",
    "    enough = SubhaloN > 30\n",
    "\n",
    "    smass = SubhaloMass[enough]\n",
    "    svmax = MaxVel[enough]\n",
    "\n",
    "    Mass_Bins = np.geomspace(np.amin(smass), np.amax(smass), num=nbins)\n",
    "    MaxVel_Bin = np.geomspace(np.amin(svmax), np.amax(svmax), num=nbins)\n",
    "\n",
    "    N_M = np.array([ (smass > mbin).sum() for mbin in Mass_Bins ])\n",
    "    N_V = np.array([ (svmax > vbin).sum() for vbin in MaxVel_Bin])\n",
    "    \n",
    "    return (Mass_Bins, N_M), (MaxVel_Bin, N_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554d98eeddfc4f518b81dffa02f9e876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data_prods/run_2cDM_L3N128_HY_power00_sigma1 already exists!\n",
      "Working on  run_2cDM_L3N128_HY_power00_sigma1\n",
      "../../data_prods/run_2cDM_L3N256_HY_power00_sigma1 already exists!\n",
      "Working on  run_2cDM_L3N256_HY_power00_sigma1\n",
      "../../data_prods/run_2cDM_L3N256_HY_power00_sigma1_dir_0 already exists!\n",
      "Working on  run_2cDM_L3N256_HY_power00_sigma1_dir_0\n",
      "../../data_prods/run_2cDM_L3N256_HY_power00_sigma1_dir_1 already exists!\n",
      "Working on  run_2cDM_L3N256_HY_power00_sigma1_dir_1\n",
      "../../data_prods/run_2cDM_L3N256_HY_power00_sigma1_dir_2 already exists!\n",
      "Working on  run_2cDM_L3N256_HY_power00_sigma1_dir_2\n",
      "../../data_prods/run_2cDM_L3N256_HY_power00_sigma1_dir_3 already exists!\n",
      "Working on  run_2cDM_L3N256_HY_power00_sigma1_dir_3\n",
      "../../data_prods/run_2cDM_L3N256_HY_power00_sigma1_dir_4 already exists!\n",
      "Working on  run_2cDM_L3N256_HY_power00_sigma1_dir_4\n",
      "../../data_prods/run_2cDM_L3N256_HY_power00_sigma1_dir_5 already exists!\n",
      "Working on  run_2cDM_L3N256_HY_power00_sigma1_dir_5\n",
      "../../data_prods/run_2cDM_L3N256_HY_power00_sigma1_dir_6 already exists!\n",
      "Working on  run_2cDM_L3N256_HY_power00_sigma1_dir_6\n",
      "../../data_prods/run_2cDM_L3N256_HY_power00_sigma1_dir_7 already exists!\n",
      "Working on  run_2cDM_L3N256_HY_power00_sigma1_dir_7\n",
      "../../data_prods/run_2cDM_L3N256_HY_power00_sigma1_dir_8 already exists!\n",
      "Working on  run_2cDM_L3N256_HY_power00_sigma1_dir_8\n",
      "../../data_prods/run_2cDM_L3N256_HY_power00_sigma1_dir_9 already exists!\n",
      "Working on  run_2cDM_L3N256_HY_power00_sigma1_dir_9\n",
      "../../data_prods/run_2cDM_L3N256_HY_powerm2m2_sigma1 already exists!\n",
      "Working on  run_2cDM_L3N256_HY_powerm2m2_sigma1\n",
      "../../data_prods/run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_0 already exists!\n",
      "Working on  run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_0\n",
      "../../data_prods/run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_1 already exists!\n",
      "Working on  run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_1\n",
      "../../data_prods/run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_2 already exists!\n",
      "Working on  run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_2\n",
      "../../data_prods/run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_3 already exists!\n",
      "Working on  run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_3\n",
      "../../data_prods/run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_4 already exists!\n",
      "Working on  run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_4\n",
      "../../data_prods/run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_5 already exists!\n",
      "Working on  run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_5\n",
      "../../data_prods/run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_6 already exists!\n",
      "Working on  run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_6\n",
      "../../data_prods/run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_7 already exists!\n",
      "Working on  run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_7\n",
      "../../data_prods/run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_8 already exists!\n",
      "Working on  run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_8\n",
      "../../data_prods/run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_9 already exists!\n",
      "Working on  run_2cDM_L3N256_HY_powerm2m2_sigma1_dir_9\n",
      "../../data_prods/run_CDM_L3N128_HY already exists!\n",
      "Working on  run_CDM_L3N128_HY\n",
      "../../data_prods/run_CDM_L3N256_HY already exists!\n",
      "Working on  run_CDM_L3N256_HY\n",
      "../../data_prods/run_CDM_L3N256_HY_dir_0 already exists!\n",
      "Working on  run_CDM_L3N256_HY_dir_0\n",
      "../../data_prods/run_CDM_L3N256_HY_dir_1 already exists!\n",
      "Working on  run_CDM_L3N256_HY_dir_1\n",
      "../../data_prods/run_CDM_L3N256_HY_dir_2 already exists!\n",
      "Working on  run_CDM_L3N256_HY_dir_2\n",
      "../../data_prods/run_CDM_L3N256_HY_dir_3 already exists!\n",
      "Working on  run_CDM_L3N256_HY_dir_3\n",
      "../../data_prods/run_CDM_L3N256_HY_dir_4 already exists!\n",
      "Working on  run_CDM_L3N256_HY_dir_4\n",
      "../../data_prods/run_CDM_L3N256_HY_dir_5 already exists!\n",
      "Working on  run_CDM_L3N256_HY_dir_5\n",
      "../../data_prods/run_CDM_L3N256_HY_dir_6 already exists!\n",
      "Working on  run_CDM_L3N256_HY_dir_6\n",
      "../../data_prods/run_CDM_L3N256_HY_dir_7 already exists!\n",
      "Working on  run_CDM_L3N256_HY_dir_7\n",
      "../../data_prods/run_CDM_L3N256_HY_dir_8 already exists!\n",
      "Working on  run_CDM_L3N256_HY_dir_8\n",
      "../../data_prods/run_CDM_L3N256_HY_dir_9 already exists!\n",
      "Working on  run_CDM_L3N256_HY_dir_9\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO:\n",
    "\n",
    "Record OmegaB and OmegaStar for each snapshot in hydro runs\n",
    "\"\"\"\n",
    "for subdir in tqdm(subdir_list):\n",
    "\n",
    "    #run name is just folder name\n",
    "    run_name = subdir.split('/')[-1]\n",
    "\n",
    "    #reject folders that aren't runs\n",
    "    if 'run' not in run_name:\n",
    "        continue\n",
    "\n",
    "    fpath = os.path.join(outdir, run_name)\n",
    "    try:\n",
    "        os.mkdir(fpath)\n",
    "    except:\n",
    "        print(f'{fpath} already exists!')\n",
    "    print(\"Working on \", run_name)\n",
    "\n",
    "    # read info from snapshots/fof\n",
    "    snaps = sorted(glob.glob(subdir + '/snap*'))\n",
    "    fofs = sorted(glob.glob(subdir + '/fof*'))\n",
    "\n",
    "    if len(snaps) == 0:\n",
    "        print(f'No snapshots for run {run_name}...')\n",
    "        continue\n",
    "\n",
    "    #get info from filename\n",
    "    splitted = run_name.split('_')\n",
    "\n",
    "    dm_type = splitted[1]\n",
    "\n",
    "    baryon_type = splitted[3]\n",
    "\n",
    "\n",
    "    run_dict = {\n",
    "        \"run name\": run_name,\n",
    "        \"DM type\": dm_type,\n",
    "        \"baryon_type\": baryon_type\n",
    "    }\n",
    "\n",
    "    if dm_type == 'CDM':\n",
    "        otherInfo = splitted[4:]\n",
    "        run_dict['otherInfo'] = otherInfo\n",
    "    elif (dm_type == '2cDM') or (dm_type == 'SIDM'):\n",
    "        powerType = splitted[4]\n",
    "        powerLaws = parsePower(powerType)\n",
    "        sigmaType = splitted[5]\n",
    "        sigma = parseSigma(sigmaType)\n",
    "        otherInfo = splitted[6:]\n",
    "        temp = {\n",
    "            \"powerLaws\": powerLaws,\n",
    "            \"sigma0\": sigma,\n",
    "            \"otherInfo\": otherInfo\n",
    "        }\n",
    "\n",
    "        for o in otherInfo:\n",
    "            if 'Vkick' in o: temp['Vkick'] = float(o.split('Vkick')[1])\n",
    "\n",
    "        run_dict.update(temp)\n",
    "\n",
    "    with h5py.File(snaps[0], 'r') as f:\n",
    "        boxSize = int((f['Header'].attrs['BoxSize'])) # in kpc\n",
    "        NPart = int(np.cbrt(f['Header'].attrs['NumPart_Total'][1]))\n",
    "        mass_resolution = [f['Header'].attrs['MassTable'][1] * 10**10] # in solar masses\n",
    "        if baryon_type == 'HY':\n",
    "            masses = f.get('PartType0/Masses')[()]\n",
    "            if np.all(np.isclose(masses, masses[0])):\n",
    "                mass_resolution.append(masses[0] * 10**10)\n",
    "        softening_length = boxSize / NPart / 29\n",
    "        temp = {\n",
    "            \"BoxSize\": boxSize,\n",
    "            \"NPart\": NPart,\n",
    "            \"Mass Resolution\": mass_resolution,\n",
    "            \"Softening Length\": softening_length\n",
    "        }\n",
    "\n",
    "        run_dict.update(temp)\n",
    "\n",
    "\n",
    "    redshifts = []\n",
    "    omega0s = []\n",
    "    file_indices = []\n",
    "    for fof in fofs:\n",
    "        i = int(Path(fof).stem.split('_')[-1])\n",
    "        file_indices.append(i)\n",
    "        with h5py.File(fof, 'r') as f:\n",
    "\n",
    "            redshifts.append( float(\"{:.2f}\".format(f['Header'].attrs['Redshift'])) )\n",
    "            omega0s.append(f['Header'].attrs['Omega0'])\n",
    "            try:\n",
    "                subhalo = f.get('Subhalo')\n",
    "                (mbins, mcount), (vbins, vcount) = get_profiles(f)\n",
    "                tname = os.path.join(fpath, \"mass_profile_{}.txt\".format(i))\n",
    "                np.savetxt(tname, (mbins, mcount))\n",
    "                tname = os.path.join(fpath, \"vel_profile_{}.txt\".format(i))\n",
    "                np.savetxt(tname, (vbins, vcount))\n",
    "                tname = os.path.join(fpath, \"subhalo_stats_{}.txt\".format(i))\n",
    "                np.savetxt(tname, ( subhalo['SubhaloVmax'], subhalo['SubhaloVmaxRad'], subhalo['SubhaloMass'], subhalo['SubhaloHalfmassRad'], subhalo['SubhaloMassInHalfRad'], subhalo['SubhaloMassInRad'], subhalo['SubhaloLen'] ) )\n",
    "\n",
    "            except KeyError:\n",
    "                continue\n",
    "    run_dict['redshifts'] = redshifts\n",
    "    run_dict['file_indices'] = file_indices\n",
    "    run_dict['Omega0'] = omega0s\n",
    "    if baryon_type == 'HY':\n",
    "        omegaBs = []\n",
    "        omegaStars = []\n",
    "        boxVol_Mpc = (boxSize / 1000)**3\n",
    "\n",
    "        for snap in snaps:\n",
    "            with h5py.File(snap, 'r') as f:\n",
    "                omegaBs.append(f.get('PartType0/Masses')[()].sum() * 10**10 / boxVol_Mpc / rho_crit)\n",
    "                try:\n",
    "                    omegaStars.append(f.get('PartType4/Masses')[()].sum() * 10**10 / boxVol_Mpc / rho_crit)\n",
    "                except:\n",
    "                    pass\n",
    "        temp = {\n",
    "            'OmegaB': omegaBs,\n",
    "            'OmegaStar': omegaStars\n",
    "        }\n",
    "        run_dict.update(temp)\n",
    "\n",
    "\n",
    "    fname = os.path.join(fpath, \"run_info.json\")\n",
    "    with open(fname, 'w') as outfile:\n",
    "        json.dump(run_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
